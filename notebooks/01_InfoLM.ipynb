{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e4fc4ca",
   "metadata": {},
   "source": [
    "# InfoLM\n",
    "\n",
    "NLG evaluation metric introduced in [InfoLM: A New Metric to Evaluate Summarization & Data2Text Generation\n",
    "](https://arxiv.org/abs/2112.01589) by Pierre Colombo, Chloe Clavel, Pablo Piantanida."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db36e1",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21674851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from nlg_eval_via_simi_measures.infolm import InfoLM\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecaeb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/bert_uncased_L-2_H-128_A-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362579b",
   "metadata": {},
   "source": [
    "## Test config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d91bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "HYPOTHESIS = [\n",
    "    \"It is a guide to action which ensures that the military always obeys the commands of the party\",\n",
    "    \"he read the book because he was interested in world history\",\n",
    "    \"he read the book because he was interested in world history\",\n",
    "    \"the cat the   cat on the mat \",\n",
    "]\n",
    "\n",
    "REFERENCES = [\n",
    "    \"It is a guide to action that ensures that the military will forever heed Party commands\",\n",
    "    \"he was interested in world history because he read the book\",\n",
    "    \"he was interested in world history because he read the book\",\n",
    "    \"the  cat is     on the mat \",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3344c1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"kl_divergence\", False, 0.25, 0.25),\n",
    "    (\"alpha_divergence\", True, 0.4, 0.3),\n",
    "    (\"beta_divergence\", False, None, 0.6),\n",
    "    (\"ab_divergence\", True, 0.25, 0.25),\n",
    "    (\"renyi_divergence\", False, 0.3, 0.1),\n",
    "    (\"l1_distance\", True, None, None),\n",
    "    (\"l2_distance\", False, None, None),\n",
    "    (\"linf_distance\", True, None, None),\n",
    "    (\"fisher_rao_distance\", False, 0.25, 0.25),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db132b44",
   "metadata": {},
   "source": [
    "## Generate test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03ae817",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/Users/stancld/Documents/projects/nlg_eval_via_simi_measures/nlg_eval_via_simi_measures/infolm.py:255: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  dict_logits_distribution[str(self.temperature)] = torch.nn.Softmax()(\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Some weights of the model checkpoint at google/bert_uncased_L-2_H-128_A-2 were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for config in configs:\n",
    "    # Reference metric\n",
    "    measure = config[0].replace(\"_divergence\", \"\").replace(\"_distance\", \"\")\n",
    "    info_lm = InfoLM(\n",
    "        model_name=MODEL_NAME,\n",
    "        measure_to_use=measure,\n",
    "        use_idf_weights=config[1],\n",
    "        alpha=config[2],\n",
    "        beta=config[3],\n",
    "    )\n",
    "    \n",
    "    if config[1] == True:\n",
    "        # batch_size = 2\n",
    "        idf_hyp, idf_ref = info_lm.prepare_idfs(HYPOTHESIS[:2], REFERENCES[:2])\n",
    "        res1 = info_lm.evaluate_batch(HYPOTHESIS[:2], REFERENCES[:2], idf_hyp, idf_ref)[measure]\n",
    "        idf_hyp, idf_ref = info_lm.prepare_idfs(HYPOTHESIS[2:], REFERENCES[2:])\n",
    "        res2 = info_lm.evaluate_batch(HYPOTHESIS[2:], REFERENCES[2:], idf_hyp, idf_ref)[measure]\n",
    "        results[f\"{config[0]}_bs2\"] = torch.tensor(res1 + res2)\n",
    "        # batch_size = 4\n",
    "        idf_hyp, idf_ref = info_lm.prepare_idfs(HYPOTHESIS, REFERENCES)\n",
    "        res = info_lm.evaluate_batch(HYPOTHESIS, REFERENCES, idf_hyp, idf_ref)[measure]\n",
    "        results[f\"{config[0]}_bs4\"] = torch.tensor(res)\n",
    "    else:\n",
    "        res = info_lm.evaluate_batch(HYPOTHESIS, REFERENCES)[measure]\n",
    "        results[config[0]] = torch.tensor(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09dd313b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kl_divergence': tensor([-2.5192, -0.0989, -0.0989, -2.1052]),\n",
       " 'alpha_divergence_bs2': tensor([-1.2924, -0.1424, -0.1424, -1.4661]),\n",
       " 'alpha_divergence_bs4': tensor([-1.3036, -0.1424, -0.1424, -1.6018]),\n",
       " 'beta_divergence': tensor([0.5291, 0.0597, 0.0597, 0.3080]),\n",
       " 'ab_divergence_bs2': tensor([5.9517, 0.5222, 0.5222, 7.0017]),\n",
       " 'ab_divergence_bs4': tensor([5.9565, 0.5222, 0.5222, 7.1950]),\n",
       " 'renyi_divergence': tensor([0.4651, 0.0425, 0.0425, 0.4088]),\n",
       " 'l1_distance_bs2': tensor([0.9679, 0.1877, 0.1877, 0.9561]),\n",
       " 'l1_distance_bs4': tensor([0.9591, 0.1877, 0.1877, 1.0823]),\n",
       " 'l2_distance': tensor([0.2053, 0.1114, 0.1114, 0.2522]),\n",
       " 'linf_distance_bs2': tensor([0.0789, 0.0869, 0.0869, 0.2324]),\n",
       " 'linf_distance_bs4': tensor([0.0777, 0.0869, 0.0869, 0.2614]),\n",
       " 'fisher_rao_distance': tensor([1.5637, 0.4957, 0.4957, 1.4570])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metrics",
   "language": "python",
   "name": "metrics"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
